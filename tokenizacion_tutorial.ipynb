{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b60e725",
   "metadata": {},
   "source": [
    "# Tokenización con Transformers\n",
    "\n",
    "Notebook para cargar un tokenizador preentrenado y explorar la tokenización de varias oraciones con Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa96cdb",
   "metadata": {},
   "source": [
    "## 1. Instalar y configurar dependencias\n",
    "\n",
    "Instalar y/o verificar la disponibilidad de `transformers` en el entorno del notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e990b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "if importlib.util.find_spec(\"transformers\") is None:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers\"])\n",
    "\n",
    "import transformers\n",
    "print(\"transformers\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd790ec",
   "metadata": {},
   "source": [
    "## 2. Cargar el tokenizador preentrenado\n",
    "\n",
    "Importar `AutoTokenizer` y cargar el modelo `sentence-transformers/all-MiniLM-L6-v2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a214bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Cargamos un tokenizador preentrenado para embeddings de oraciones\n",
    "# Este se usará para convertir texto a tokens y sus IDs correspondientes\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c9449",
   "metadata": {},
   "source": [
    "## 3. Definir función de exploración de tokenización\n",
    "\n",
    "Implementar `explore_tokenization(text)` para tokenizar, codificar a IDs e imprimir la correspondencia token→ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcedaf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_tokenization(text):\n",
    "    \"\"\"\n",
    "    Esta función muestra cómo el tokenizador transforma un texto dado en tokens e IDs,\n",
    "    imprimiendo información detallada para su análisis.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nAnalizando: '{text}'\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Tokeniza el texto (convierte string a lista de strings/subpalabras)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Codifica el texto a IDs (convierte string a lista de números)\n",
    "    # Nota: encode agrega automáticamente tokens especiales al inicio y final\n",
    "    tokens_ids = tokenizer.encode(text)\n",
    "\n",
    "    print(f\"\\nTexto original: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Número de tokens: {len(tokens)}\")\n",
    "    print(f\"IDs de tokens: {tokens_ids}\")\n",
    "\n",
    "    print(\"\\nCorrespondencia Token → ID:\")\n",
    "\n",
    "    # Excluimos [CLS] (inicio) y [SEP] (fin) al imprimir la correspondencia manual\n",
    "    # tokens_ids[1:-1] toma los IDs del contenido real\n",
    "    for i, (token, token_id) in enumerate(zip(tokens, tokens_ids[1:-1])):\n",
    "        print(f\"  {i+1}. '{token}' → {token_id}\")\n",
    "\n",
    "    return tokens, tokens_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2133c0",
   "metadata": {},
   "source": [
    "## 4. Preparar ejemplos y almacenamiento de resultados\n",
    "\n",
    "Crear la lista `examples` y el diccionario `tokenization_results` para guardar tokens e IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"The sun is shining brightly through the window this morning\",\n",
    "    \"She loves to read books about history and ancient civilizations\",\n",
    "    \"It feels so cold outside that my hands are starting to go numb\",\n",
    "    \"They won the game after playing an intense and challenging match\",\n",
    "]\n",
    "\n",
    "# Diccionario para almacenar los resultados de tokenización\n",
    "tokenization_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c99fd",
   "metadata": {},
   "source": [
    "## 5. Ejecutar la tokenización y revisar salidas\n",
    "\n",
    "Iterar sobre los ejemplos, llamar a `explore_tokenization` y almacenar resultados en el diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PARTE 1: ENTENDIENDO LOS TOKENS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Procesamos cada oración de ejemplo y almacenamos tokens e IDs\n",
    "for example in examples:\n",
    "    tokens, tokens_ids = explore_tokenization(example)\n",
    "    tokenization_results[example] = {\"tokens\": tokens, \"ids\": tokens_ids}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
