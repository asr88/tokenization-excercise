{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b60e725",
   "metadata": {},
   "source": [
    "# Tokenización con Transformers\n",
    "\n",
    "Notebook para cargar un tokenizador preentrenado y explorar la tokenización de varias oraciones con Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa96cdb",
   "metadata": {},
   "source": [
    "## 1. Instalar y configurar dependencias\n",
    "\n",
    "Instalar y/o verificar la disponibilidad de `transformers` en el entorno del notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e990b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers 4.57.5\n"
     ]
    }
   ],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "if importlib.util.find_spec(\"transformers\") is None:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers\"])\n",
    "\n",
    "import transformers\n",
    "print(\"transformers\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd790ec",
   "metadata": {},
   "source": [
    "## 2. Cargar el tokenizador preentrenado\n",
    "\n",
    "Importar `AutoTokenizer` y cargar el modelo `sentence-transformers/all-MiniLM-L6-v2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a214bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Cargamos un tokenizador preentrenado para embeddings de oraciones\n",
    "# Este se usará para convertir texto a tokens y sus IDs correspondientes\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c9449",
   "metadata": {},
   "source": [
    "## 3. Definir función de exploración de tokenización\n",
    "\n",
    "Implementar `explore_tokenization(text)` para tokenizar, codificar a IDs e imprimir la correspondencia token→ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcedaf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_tokenization(text):\n",
    "    \"\"\"\n",
    "    Esta función muestra cómo el tokenizador transforma un texto dado en tokens e IDs,\n",
    "    imprimiendo información detallada para su análisis.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nAnalizando: '{text}'\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Tokeniza el texto (convierte string a lista de strings/subpalabras)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Codifica el texto a IDs (convierte string a lista de números)\n",
    "    # Nota: encode agrega automáticamente tokens especiales al inicio y final\n",
    "    tokens_ids = tokenizer.encode(text)\n",
    "\n",
    "    print(f\"\\nTexto original: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Número de tokens: {len(tokens)}\")\n",
    "    print(f\"IDs de tokens: {tokens_ids}\")\n",
    "\n",
    "    print(\"\\nCorrespondencia Token → ID:\")\n",
    "\n",
    "    # Excluimos [CLS] (inicio) y [SEP] (fin) al imprimir la correspondencia manual\n",
    "    # tokens_ids[1:-1] toma los IDs del contenido real\n",
    "    for i, (token, token_id) in enumerate(zip(tokens, tokens_ids[1:-1])):\n",
    "        print(f\"  {i+1}. '{token}' → {token_id}\")\n",
    "\n",
    "    return tokens, tokens_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2133c0",
   "metadata": {},
   "source": [
    "## 4. Preparar ejemplos y almacenamiento de resultados\n",
    "\n",
    "Crear la lista `examples` y el diccionario `tokenization_results` para guardar tokens e IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc7f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"The sun is shining brightly through the window this morning\",\n",
    "    \"She loves to read books about history and ancient civilizations\",\n",
    "    \"It feels so cold outside that my hands are starting to go numb\",\n",
    "    \"They won the game after playing an intense and challenging match\",\n",
    "]\n",
    "\n",
    "# Diccionario para almacenar los resultados de tokenización\n",
    "tokenization_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1c99fd",
   "metadata": {},
   "source": [
    "## 5. Ejecutar la tokenización y revisar salidas\n",
    "\n",
    "Iterar sobre los ejemplos, llamar a `explore_tokenization` y almacenar resultados en el diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4a727b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PARTE 1: ENTENDIENDO LOS TOKENS\n",
      "============================================================\n",
      "\n",
      "Analizando: 'The sun is shining brightly through the window this morning'\n",
      "--------------------------------------------------\n",
      "\n",
      "Texto original: The sun is shining brightly through the window this morning\n",
      "Tokens: ['the', 'sun', 'is', 'shining', 'brightly', 'through', 'the', 'window', 'this', 'morning']\n",
      "Número de tokens: 10\n",
      "IDs de tokens: [101, 1996, 3103, 2003, 9716, 14224, 2083, 1996, 3332, 2023, 2851, 102]\n",
      "\n",
      "Correspondencia Token → ID:\n",
      "  1. 'the' → 1996\n",
      "  2. 'sun' → 3103\n",
      "  3. 'is' → 2003\n",
      "  4. 'shining' → 9716\n",
      "  5. 'brightly' → 14224\n",
      "  6. 'through' → 2083\n",
      "  7. 'the' → 1996\n",
      "  8. 'window' → 3332\n",
      "  9. 'this' → 2023\n",
      "  10. 'morning' → 2851\n",
      "\n",
      "Analizando: 'She loves to read books about history and ancient civilizations'\n",
      "--------------------------------------------------\n",
      "\n",
      "Texto original: She loves to read books about history and ancient civilizations\n",
      "Tokens: ['she', 'loves', 'to', 'read', 'books', 'about', 'history', 'and', 'ancient', 'civilizations']\n",
      "Número de tokens: 10\n",
      "IDs de tokens: [101, 2016, 7459, 2000, 3191, 2808, 2055, 2381, 1998, 3418, 24784, 102]\n",
      "\n",
      "Correspondencia Token → ID:\n",
      "  1. 'she' → 2016\n",
      "  2. 'loves' → 7459\n",
      "  3. 'to' → 2000\n",
      "  4. 'read' → 3191\n",
      "  5. 'books' → 2808\n",
      "  6. 'about' → 2055\n",
      "  7. 'history' → 2381\n",
      "  8. 'and' → 1998\n",
      "  9. 'ancient' → 3418\n",
      "  10. 'civilizations' → 24784\n",
      "\n",
      "Analizando: 'It feels so cold outside that my hands are starting to go numb'\n",
      "--------------------------------------------------\n",
      "\n",
      "Texto original: It feels so cold outside that my hands are starting to go numb\n",
      "Tokens: ['it', 'feels', 'so', 'cold', 'outside', 'that', 'my', 'hands', 'are', 'starting', 'to', 'go', 'numb']\n",
      "Número de tokens: 13\n",
      "IDs de tokens: [101, 2009, 5683, 2061, 3147, 2648, 2008, 2026, 2398, 2024, 3225, 2000, 2175, 15903, 102]\n",
      "\n",
      "Correspondencia Token → ID:\n",
      "  1. 'it' → 2009\n",
      "  2. 'feels' → 5683\n",
      "  3. 'so' → 2061\n",
      "  4. 'cold' → 3147\n",
      "  5. 'outside' → 2648\n",
      "  6. 'that' → 2008\n",
      "  7. 'my' → 2026\n",
      "  8. 'hands' → 2398\n",
      "  9. 'are' → 2024\n",
      "  10. 'starting' → 3225\n",
      "  11. 'to' → 2000\n",
      "  12. 'go' → 2175\n",
      "  13. 'numb' → 15903\n",
      "\n",
      "Analizando: 'They won the game after playing an intense and challenging match'\n",
      "--------------------------------------------------\n",
      "\n",
      "Texto original: They won the game after playing an intense and challenging match\n",
      "Tokens: ['they', 'won', 'the', 'game', 'after', 'playing', 'an', 'intense', 'and', 'challenging', 'match']\n",
      "Número de tokens: 11\n",
      "IDs de tokens: [101, 2027, 2180, 1996, 2208, 2044, 2652, 2019, 6387, 1998, 10368, 2674, 102]\n",
      "\n",
      "Correspondencia Token → ID:\n",
      "  1. 'they' → 2027\n",
      "  2. 'won' → 2180\n",
      "  3. 'the' → 1996\n",
      "  4. 'game' → 2208\n",
      "  5. 'after' → 2044\n",
      "  6. 'playing' → 2652\n",
      "  7. 'an' → 2019\n",
      "  8. 'intense' → 6387\n",
      "  9. 'and' → 1998\n",
      "  10. 'challenging' → 10368\n",
      "  11. 'match' → 2674\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PARTE 1: ENTENDIENDO LOS TOKENS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Procesamos cada oración de ejemplo y almacenamos tokens e IDs\n",
    "for example in examples:\n",
    "    tokens, tokens_ids = explore_tokenization(example)\n",
    "    tokenization_results[example] = {\"tokens\": tokens, \"ids\": tokens_ids}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
