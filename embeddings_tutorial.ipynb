{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d1ad6c",
   "metadata": {},
   "source": [
    "# Embeddings de oraciones con Transformers\n",
    "\n",
    "Notebook para generar embeddings de palabras y oraciones usando `sentence-transformers/all-MiniLM-L6-v2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18117862",
   "metadata": {},
   "source": [
    "## 1. Importar librerías y cargar el modelo\n",
    "\n",
    "Importamos `AutoModel`, `AutoTokenizer` y configuramos el modelo preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d00a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Cargamos un tokenizador preentrenado para embeddings de oraciones\n",
    "# Este se usará para convertir texto a tokens y sus IDs correspondientes\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Cargamos el modelo preentrenado para generar embeddings de oraciones\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acea1286",
   "metadata": {},
   "source": [
    "## 2. Definir mean pooling y función de embedding\n",
    "\n",
    "Implementamos `mean_pooling` y `get_embedding` para obtener embeddings normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f51ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de Mean Pooling\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"\n",
    "    Promedia los embeddings de los tokens de una secuencia, ignorando el padding.\n",
    "    Retorna un único vector por cada oración.\n",
    "    \"\"\"\n",
    "\n",
    "    token_embeddings = model_output[0]  # Contiene todos los embeddings de los tokens\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "\n",
    "    # Suma ponderada de embeddings / Cantidad de tokens reales\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )\n",
    "\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"\n",
    "    Esta función obtiene el embedding normalizado de una oración dada\n",
    "    utilizando el modelo cargado y la función de mean pooling.\n",
    "    \"\"\"\n",
    "\n",
    "    # Codifica el texto a tensores de entrada para el modelo\n",
    "    encoded_input = tokenizer([text], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Desactiva gradientes para eficiencia\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Aplica mean pooling para obtener el embedding de la oración\n",
    "    sentence_embedding = mean_pooling(model_output, encoded_input[\"attention_mask\"])\n",
    "\n",
    "    # Normaliza el embedding\n",
    "    sentence_embedding = F.normalize(sentence_embedding, p=2, dim=1)\n",
    "\n",
    "    return sentence_embedding[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0dac33",
   "metadata": {},
   "source": [
    "## 3. Preparar ejemplos de palabras y frases\n",
    "\n",
    "Definimos listas de palabras y frases para calcular embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05472017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de palabras de ejemplos para obtener embeddings\n",
    "example_words = [\n",
    "    \"cat\",\n",
    "    \"dog\",\n",
    "    \"lion\",\n",
    "    \"red\",\n",
    "    \"blue\",\n",
    "    \"green\",\n",
    "    \"code\",\n",
    "    \"python\",\n",
    "    \"java\",\n",
    "    \"happy\",\n",
    "    \"sad\",\n",
    "    \"angry\",\n",
    "    \"pizza\",\n",
    "    \"hamburger\",\n",
    "    \"pasta\",\n",
    "]\n",
    "\n",
    "# Lista de frases de ejemplo para obtener embeddings\n",
    "example_phrases = [\n",
    "    \"The sun is shining brightly through the window this morning.\",\n",
    "    \"She loves to read books about history and ancient civilizations.\",\n",
    "    \"It feels so cold outside that my hands are starting to go numb.\",\n",
    "    \"They won the game after playing an intense and challenging match.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dea131",
   "metadata": {},
   "source": [
    "## 4. Calcular embeddings\n",
    "\n",
    "Calculamos embeddings para palabras y frases, y mostramos dimensiones y algunas posiciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23826373",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_words = {}\n",
    "embeddings_matrix_words = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"EMBEDDINGS PARA PALABRAS INDIVIDUALES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Obtenemos y mostramos embeddings para palabras individuales\n",
    "for word in example_words:\n",
    "    embedding = get_embedding(word)\n",
    "    embeddings_words[word] = embedding\n",
    "    embeddings_matrix_words.append(embedding)\n",
    "    print(f\"\\nPalabra: '{word}' → Vector de {len(embedding)} dimensiones\")\n",
    "    print(f\"Primeras 5 dimensiones del embedding: {embedding[:5]}\")\n",
    "\n",
    "embeddings_phrases = {}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"EMBEDDINGS PARA ORACIONES O FRASES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Obtenemos y mostramos embeddings para frases\n",
    "for phrase in example_phrases:\n",
    "    embedding = get_embedding(phrase)\n",
    "    embeddings_phrases[phrase] = embedding\n",
    "    print(f\"\\nFrase: '{phrase}' → Vector de {len(embedding)} dimensiones\")\n",
    "    print(f\"Primeras 5 dimensiones del embedding: {embedding[:5]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
